name: GPU Workflow Example

on:
  push:
    branches:
      - main

jobs:
  gpu_job:
    runs-on: ubuntu-latest  # 默认使用 GitHub 托管的虚拟机，没有 GPU

    container:
      image: nvidia/cuda:11.2-base  # 使用包含 CUDA 和 NVIDIA 支持的 Docker 镜像
      options: --gpus all  # 启用所有可用的 GPU

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up NVIDIA Driver
        run: |
          sudo apt-get update
          sudo apt-get install -y nvidia-driver-460  # 安装 NVIDIA 驱动

      - name: Check GPU availability
        run: |
          nvidia-smi  # 检查 GPU 是否可用

      - name: Install dependencies
        run: |
          sudo apt-get install -y python3-pip
          pip install -r requirements.txt

      - name: Run model inference
        run: |
          python run_inference.py  # 运行你的推理脚本

  # 如果你有自托管的 GPU 服务器，也可以用以下方式设置
  self_hosted_gpu:
    runs-on: [self-hosted, gpu]  # 使用自托管的 GPU 机器
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Check GPU status
        run: |
          nvidia-smi  # 查看 GPU 状态

      - name: Run model inference on GPU
        run: |
          python run_inference.py
